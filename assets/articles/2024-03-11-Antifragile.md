# Afterthought: Antifragile by Nassim Nicholas Taleb
afterthought

One's one of the person who mistaken robust and antifragile before one read the book. One don't know anything about the author before fs.blog introduced one to. To pick up this 600+ page book (very long, compared to an average of 300+ pages one read), it takes a will to construct and simplify an existing project that one constructed quite messy, and after some simplification, viol√†, here is one. 

To understand antifragile, think about positive, neutral, and negative. Fragile is the negative, antifragile is the positive, and robust is neutral. For fragile means it breaks if you do something to it outside creator's expectation. For example, if you ever fill in a form, like the one that asks for your cover letter, you technically could inject malicious code, so that when the recruiter opens it, bam, his/her computer breaks down. Nowadays, you probably can't do that anymore, because a "HTML sanitizer" is in place to stripped out the malicious code before sending it to the recruiter. 

Something robust means it's resistant to break. E.g. if you build something to spend to space, you need to think about all the possible stuffs that could happen so you can remotely ask it to do what in case what happens. If your rover is already on Mars, you can't possibly fly to mars, get the rover back, fix it, and resend it back to Mars, can you? Keep in mind, it takes more than 2 years to travel to Mars. 

Something antifragile grows if you try to break it. Meaning, it learns with every "mistake" or "meddling" that you try, to an extent. Usually, that applies for living organisms, but also to decentralized organizations (operated by humans), and other stuffs. In the future, we might see AI to be antifragile, but one don't know how long it'll take to such stage. 

In some case, antifragile is at the expense of others. E.g. exercise may kill your weak cells, leaving the strong cells to replace them. E.g. chemotherapy may kill your weak cancerous cells, leaving stronger resistant cancerous cell to spread. E.g. for a large organization to survive, it needs to layoff its incompetent employees and hire competent ones. 

**Touristification** is the removal of uncertainty and randomness from things. In _Superforecasting_, we see that human wants certainty, for we need energy to keep something in constant reminder, plus the release of "fight-or-flight" chemicals that keeps us alert. Alas, our aim for stability makes the system less stable, ironically. 

One thing that opens one mind is the illusion of stable work. Our stability of salary comes at the hidden risk of layoff. Increment of salary are done in step (when you get up the ladder hierarchy); while the only fluctuation you get are in commissions. On the other hand, self-employed may not get lots of money, but there are no bosses to fire them either. If you don't get enough for the week, it may be time to revise your strategy on how to earn money, or where clients would go more often (so you work there instead), etc. 

Of course, one don't totally agree with the jumps. In fact, some stuffs like history aggregates slowly, so it's not a flat horizontal line, but a zig-zag upward-slope or downward-slope line. The "jump" is an oversimplification, in one's idea concerning history. E.g. if world war III is going to happen, it likely don't happen overnight, but Russia have to start fighting Ukraine first, then Iran have to fight Hamas first, then other country get into fight first, before there are fights all over the world. One do agree though, that United States trying to suppress such violence, and people striking for "peace" when they don't even know anything about except for being slavery to biases and emotions, cause such suppression to reach a threshold point where it could not be suppress anymore, and hence outbreak; just like you keep pumping air into a balloon without letting air out, it'll reach a point and pop. 

Author introduced something called _stochastic resonance_, where injecting random noise into a system improve its signal. Mathematics is called _simulated annealing_, and we see quite a lot of applications in it. E.g. one used to see this annealing in quantum computing, where without a real quantum computer, you could use annealing to search for the shortest path travel (like GPS) between two points joined by routes. Then, if you're a fad of machine learning, lots of ML models used injection of noise to train better algorithms. One of them quite popular recently is called Stable Diffusion, which replaces the Generative Adversarial Network (GAN) for generating images, and you can use Diffusion model to convert text to image, like that used in OpenAI's DALL-E model. Basically, it trains by converting an image, subsequently adding noise into the image with every cycle, until it's completely random. Then, it tries to reconstruct from that noisy image, back to the picture. Try DALL-E and you'll know how good it is. 

We always want to change stuffs, to fix things, regardless of whether the thing needs fixing or not. Author argues that such fixing could cause catastrophe, as something that might recover itself, after you fix, become irrecoverable in a worse-than-before-the-fix state. Therefore, one should first determine whether the thing needs fixing before coming up with a solution to fix it. 

In _Superforecasting_, we deal with probabilities which is what this book is against. In one's opinion, there aren't anything clash between this book and that. Reason: Superforecasting tries to get an accurate forecast of certain forecast-able events (non-Black-Swan) to a high accuracy. This book argues that, instead of trying to forecast, which is difficult, we should try to build something that's highly probable (what Superforecasting called "too easy to be forecast that they don't need to be forecast"), or highly improbable, so we don't need superforecasters to forecast events, but could be highly sure/unsure of its happening. One doesn't change the existing system, this book encourages you to change the system. 

Seriously speaking, one haven't read that much of Seneca; for most translations are using twisted English that's quite difficult to understand without extreme concentration. Nor do one know Greek nor Latin, so the original text requires some while-learning-while-reading tremendous effort (though doable). 

Anyways, author demonstrated the Seneca's barbell, which is an extremization between both ends. Example, having a stable job while gambling in extremely risky but high return areas, in 90/10 (or at most 80/20, if you're prepared to lose 20%). This isn't the first time the technique reached one's ear. In _Originals_ by Adam Grant, his students continued study (stability) (#no-college-dropout) while preparing to sell glasses via online platform (it's very possible no one wants to buy short-sighted glasses via online platform, no one knows or tried at that time, so the risk is high). And one agree, it's something to use even without superforecasting. 

When we can **_choose_** to do or not do something, our minds are free. When you're forced to do something, and you have no choice but to do that thing, it's fragile. With a choice, you can always choose the alternative if you don't like it, or for whatever reasons, hence antifragile. Choice allows one to fall back on something, when things did not work out as we expect it to. Just imagine, if you only have one path to the destination, and that path is block, there are no other ways to take around, hence you can never reach your destination unless the obstacle is removed. That's why it's fragile. On the other hand, with many pathways toward the destination, you can always get to it, even if multiple pathways are blocked; therefore, antifragile. 

Especially when your outcome has very little loss, and a lot of gain. The loss is so little it's bearable, even after multiple trials; while the gain is so large that it's worth to bear the loss, even if at the end of the day, you didn't hit it. If you're rational enough, on top of such asymmetry, you'd choose something that has a higher probability of getting the hit, and that's what makes guarantees. 

Then, we think that study comes before doing. Wrong! Just think, how do the people write the textbook you're studying without people first doing, then compile them into a textbook? To get things work, people do trial and error; then only later looked back to figure out how the heck does it work. Those who got it working will tell you they don't need to understand the why, they just need the thing working. 

Then, two things aren't necessarily the same thing. For example, we may think that war causes oil prices to rise; but oil prices would alter itself according to our prediction; so we predict it to rise, everyone would be selling, then it'll drop, contrary to what we predict. So, oil prices depends on our prediction and trader's actions, rather than the war. 

NASA used to have the O-rings problem. After that, mentioned in Adam Grant's _Think Again_, they need to prove themselves to proceed with the launch, instead of those opposing the launch proving themselves to not launch; for launch is more catastrophic if there's a problem compared with a launch delayed. This idea linked to the _via negativa_ effect that author mentioned. Doctors should prove their intervention valid, with sufficient benefits to overcome hidden/unseen detriments, rather than patients proving themselves they're better without taking intervention. 

---
(Chapter 22 onwards not yet read)


### Notes: 
1. That is not to say that WW III will certainly happen. There is a chance, but unfortunately, it's a Black Swan, not forecast-able. What's forecast-able is whether state A and state B (like US and China) would go to war or not (but alas, one isn't a superforecaster yet, so that one can't forecast), or whether two stages (Israel and Hamas) would stop war or not. 

